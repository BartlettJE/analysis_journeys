[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis Journeys",
    "section": "",
    "text": "Overview\nThis book contains data analysis journeys to practice your wrangling, visualisation, and analysis skills in a less structured format than the main PsyTeachR books. We have designed these tasks as a bridge between the structured learning in the core course book chapters and your assessments.\nWe present you with a new data set, show you what the end product should look like, and see if you can apply your data wrangling, visualisation, and/or analysis skills to get there. They apply across all levels and our PsyTeachR books, but your lecturer will point you to the most appropriate chapters for which skills you have developed.\nAs you gain independence, this is the crucial skill. Data analysis is all about seeing the data you have available to you and identifying what the end product needs to be to apply your visualisation and analysis techniques. You can then mentally (or physically) create a checklist of tasks to work backwards to get there. There might be a lot of trial and error as you try one thing, it does not quite work, so you go back and try something else. If you get stuck though, we have a range of hints and task lists that you can unhide, then the solution to check your attempts against.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "Bartlett-analysis-journey-wrangle.html",
    "href": "Bartlett-analysis-journey-wrangle.html",
    "title": "1  Bartlett et al. (2022): Attentional bias in smokers",
    "section": "",
    "text": "1.1 Task preparation",
    "crumbs": [
      "Wrangling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bartlett et al. (2022): Attentional bias in smokers</span>"
    ]
  },
  {
    "objectID": "Bartlett-analysis-journey-wrangle.html#task-preparation",
    "href": "Bartlett-analysis-journey-wrangle.html#task-preparation",
    "title": "1  Bartlett et al. (2022): Attentional bias in smokers",
    "section": "",
    "text": "1.1.1 Introduction to the data set\nFor this task, we are using open data from Bartlett et al. (2022). The abstract of their article is:\n\nBoth daily and non-daily smokers find it difficult to quit smoking long-term. One factor associated with addictive behaviour is attentional bias, but previous research in daily and non-daily smokers found inconsistent results and did not report the reliability of their cognitive tasks. Using an online sample, we compared daily (n = 106) and non-daily (n = 60) smokers in their attentional bias towards smoking pictures. Participants completed a visual probe task with two picture presentation times: 200ms and 500ms. In confirmatory analyses, there were no significant effects of interest, and in exploratory analyses, equivalence testing showed the effects were statistically equivalent to zero. The reliability of the visual probe task was poor, meaning it should not be used for repeated testing or investigating individual differences. The results can be interpreted in line with contemporary theories of attentional bias where there are unlikely to be stable trait-like differences between smoking groups. Future research in attentional bias should focus on state-level differences using more reliable measures than the visual probe task.\n\nTo summarise, they compared two daily and non-daily smokers on something called attentional bias. This is the idea that when people use drugs often, things associated with those drugs grab people’s attention.\nTo measure attentional bias, participants completed a dot probe task. This is a computer task where participants see two pictures side-by-side: one related to smoking like someone holding a cigarette and one unrelated to smoking like someone holding a fork. Both the images disappear and a small dot appears in the location of one of the images. Participants must press left or right on the keyboard to identify where the dot appeared. This process is repeated many times for different images, different locations of the dot, and different durations of showing the images. The idea is if smoking images grab people’s attention, they will be able to identify the dot location faster on average when it appears in the location of the smoking images compared to when it appears in the location of the the non-smoking images.\nResponse time tasks like this are incredibly common in psychology and cognitive neuroscience, and being able to wrangle hundreds of trials is a great demonstration of your new data skills. After setting up your files and project for the chapter, we will outline the kind of problems you are trying to solve.\n\n1.1.2 Organising your files and project for the task\nBefore we can get started, you need to organise your files and project for the task, so your working directory is in order.\n\nYou can create a folder for the data analysis journeys book and a new sub-folder for this chapter, something like: analysis_journeys, create a new folder for this chapter called something like Bartlett_wrangling. Within Bartlett_wrangling, create two new folders called data and figures.\nCreate an R Project for Bartlett_wrangling as an existing directory for your chapter folder. This should now be your working directory.\nCreate a new R Markdown/Quarto document and give it a sensible title describing the chapter, such as Bartlett et al. (2022): Data Wrangling. Delete everything below line 10 so you have a blank file to work with and save the file in your Bartlett_wrangling folder.\nWe are working with data separated into two files. The links are data file one (Bartlett_demographics.csv) and data file two (Bartlett_trials.csv). Right click the links and select “save link as”, or clicking the links will save the files to your Downloads. Make sure that both files are saved as “.csv”. Save or copy the file to your data/ folder within Bartlett_wrangling.\n\nYou are now ready to start working on the task!",
    "crumbs": [
      "Wrangling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bartlett et al. (2022): Attentional bias in smokers</span>"
    ]
  },
  {
    "objectID": "Bartlett-analysis-journey-wrangle.html#overview",
    "href": "Bartlett-analysis-journey-wrangle.html#overview",
    "title": "1  Bartlett et al. (2022): Attentional bias in smokers",
    "section": "\n1.2 Overview",
    "text": "1.2 Overview\n\n1.2.1 Load tidyverse and read the data files\nBefore we explore what wrangling we need to do, load tidyverse and read the two data files. As a prompt, save the data files to these object names to be consistent with the activities below, but you can check the solution if you are stuck.\n\n# Load the tidyverse package below\nlibrary(tidyverse)\n\n# Load the data files\n# This should be the Bartlett_demographics.csv file \ndemog &lt;- ?\n\n# This should be the Bartlett_trials.csv file \ntrials &lt;- ?\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nYou should have the following in a code chunk:\n\n# Load the tidyverse package below\nlibrary(tidyverse)\n\n# Load the data files\n# This should be the Bartlett_demographics.csv file \ndemog &lt;- read_csv(\"data/Bartlett_demographics.csv\")\n\n# This should be the Bartlett_trials.csv file \ntrials &lt;- read_csv(\"data/Bartlett_trials.csv\")\n\n\n\n\n\n1.2.2 Explore demog and trials\n\nThe data from Bartlett et al. (2022) is split into two data files. In demog, we have the participant ID (participant_private_id) and several demographic variables.\n\n\nRows: 205\nColumns: 20\n$ participant_private_id &lt;dbl&gt; 631737, 631738, 631741, 631739, 631749, 631746,…\n$ consent_given          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age                    &lt;dbl&gt; 46, 54, 23, 34, 38, 19, 25, 21, 28, 35, 47, 45,…\n$ cigarettes_per_week    &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n$ smoke_everyday         &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n$ past_four_weeks        &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n$ age_started_smoking    &lt;chr&gt; \"12\", \"17\", \"17\", \"16\", \"15\", \"16\", \"22\", \"11\",…\n$ country_of_origin      &lt;chr&gt; \"United Kingdom\", \"Germany\", \"Poland\", \"Austral…\n$ cpd                    &lt;chr&gt; \"6\", \"5\", \"10\", \"20\", \"10\", \"1\", \"6\", \"15\", \"12…\n$ ethnicity              &lt;chr&gt; \"White / Caucasian\", \"White / Caucasian\", \"Mixe…\n$ gender                 &lt;chr&gt; \"Female\", \"Female\", \"Male\", \"Female\", \"Female\",…\n$ last_cigarette         &lt;chr&gt; \"60\", \"780\", \"90\", \"5\", \"60\", \"720\", \"10\", \"10\"…\n$ level_education        &lt;chr&gt; \"Graduated University / College\", \"Graduated Un…\n$ technical_issues       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ FTND_1                 &lt;dbl&gt; 2, 0, 0, 3, 3, 0, 1, 2, 1, 3, 2, 2, 2, 1, 2, 0,…\n$ FTND_2                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,…\n$ FTND_3                 &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,…\n$ FTND_4                 &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,…\n$ FTND_5                 &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,…\n$ FTND_6                 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,…\n\n\nThe columns (variables) we have in the data set are:\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\nparticipant_private_id\ndouble\nParticipant number.\n\n\nconsent_given\ndouble\n1 = informed consent, 2 = no consent.\n\n\nage\ndouble\nAge in Years.\n\n\ncigarettes_per_week\ncharacter\nDo you smoke every week? Yes or No.\n\n\nsmoke_everyday\ncharacter\nDo you smoke everyday? Yes or No.\n\n\npast_four_weeks\ncharacter\nHave you smoked in the past four weeks? Yes or No.\n\n\nage_started_smoking\ncharacter\nAge started smoking in years.\n\n\ncountry_of_origin\ncharacter\nCountry of origin.\n\n\ncpd\ncharacter\nHow many cigarettes do you smoke per day?\n\n\nethnicity\ncharacter\nWhat is your ethniciity?\n\n\ngender\ncharacter\nWhat is your gender?\n\n\nlast_cigarette\ncharacter\nHow long in minutes since your last cigarette?\n\n\nlevel_education\ncharacter\nWhat is your highest level of education?\n\n\ntechnical_issues\ncharacter\nDid you experience any technical issues? Yes or No\n\n\nFTND_1 to FTND_6\ndouble\nSix items of the Fagerstrom Test for Nicotine Dependence\n\n\n\nIn trials, we then have the participant ID (participant_private_id) and trial-by-trial information from the software Gorilla (an online experiment service).\n\n\nRows: 244,847\nColumns: 15\n$ participant_private_id &lt;dbl&gt; 631737, 631737, 631737, 631737, 631737, 631737,…\n$ trial_number           &lt;chr&gt; \"BEGIN TASK\", \"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\"…\n$ reaction_time          &lt;dbl&gt; NA, 8007.015, 249.823, 199.765, 1999.671, 249.8…\n$ response               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ correct                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ display                &lt;chr&gt; NA, \"instructions\", \"practice\", \"practice\", \"pr…\n$ answer                 &lt;chr&gt; NA, NA, \"right\", \"right\", \"right\", \"left\", \"lef…\n$ soa                    &lt;dbl&gt; NA, NA, 200, 200, 200, 200, 200, 200, 500, 500,…\n$ screen_name            &lt;chr&gt; NA, \"instructions_continue\", \"Screen 1\", \"Scree…\n$ image_left             &lt;chr&gt; NA, NA, \"p1.jpg\", \"p1.jpg\", \"p1.jpg\", \"p1.jpg\",…\n$ image_right            &lt;chr&gt; NA, NA, \"p2.jpg\", \"p2.jpg\", \"p2.jpg\", \"p2.jpg\",…\n$ dot_left               &lt;chr&gt; NA, NA, \"nodot.jpg\", \"nodot.jpg\", \"nodot.jpg\", …\n$ dot_right              &lt;chr&gt; NA, NA, \"dot.jpg\", \"dot.jpg\", \"dot.jpg\", \"nodot…\n$ trial_type             &lt;chr&gt; NA, NA, \"practice\", \"practice\", \"practice\", \"pr…\n$ block                  &lt;dbl&gt; NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nThe columns (variables) we have in the data set are:\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\nparticipant_private_id\ndouble\nParticipant number.\n\n\ntrial_number\ncharacter\nTrial number as an integer, plus start and end task.\n\n\nreaction_time\ndouble\nParticipant response time in milliseconds (ms)\n\n\nresponse\ncharacter\nKeyboard response from participant. Left or Right.\n\n\ncorrect\ndouble\nWas the response correct? 1 = correct, 0 = incorrect.\n\n\ndisplay\ncharacter\nTrial display: e.g., practice, trials, instructions, breaks.\n\n\nanswer\ncharacter\nWhat is the correct answer? Left or Right.\n\n\nsoa\ndouble\nStimulus onset asynchrony. How long the images were shown for: 200ms or 500ms.\n\n\nscreen_name\ncharacter\nName of the screen: e.g., screen 1, fixation, stimuli, response.\n\n\nimage_left\ncharacter\nName of the image file in the left area.\n\n\nimage_right\ncharacter\nName of the image file in the right area.\n\n\ndot_left\ncharacter\nName of the dot image file in the left area.\n\n\ndot_right\ncharacter\nName of the dot image file in the right area.\n\n\ntrial_type\ncharacter\nCategory of the trial: practice, neutral, nonsmoking, smoking.\n\n\nblock\ndouble\nNumber of the trial block.\n\n\n\n\n\n\n\n\n\nTry this\n\n\n\nNow we have introduced the two data sets, explore them using different methods we introduced. For example, opening the data objects as a tab to scroll around, explore with glimpse(), or even try plotting some of the variables to see what they look like using visualisation skills from Chapter 3.\nDo you notice any variables that look the wrong type? Can you see any responses in there that are going to cause problems?",
    "crumbs": [
      "Wrangling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bartlett et al. (2022): Attentional bias in smokers</span>"
    ]
  },
  {
    "objectID": "Bartlett-analysis-journey-wrangle.html#wrangling-demographics",
    "href": "Bartlett-analysis-journey-wrangle.html#wrangling-demographics",
    "title": "1  Bartlett et al. (2022): Attentional bias in smokers",
    "section": "\n1.3 Wrangling demographics",
    "text": "1.3 Wrangling demographics\nFor this kind of data, we recommend wrangling each file first, before joining them together. Starting with the demographics file, there are a few wrangling steps before the data are ready to summarise. We are going to show you a preview of the starting data set and the end product we are aiming for.\n\n\nRaw data\nWrangled data\n\n\n\n\n\nRows: 205\nColumns: 20\n$ participant_private_id &lt;dbl&gt; 631737, 631738, 631741, 631739, 631749, 631746,…\n$ consent_given          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age                    &lt;dbl&gt; 46, 54, 23, 34, 38, 19, 25, 21, 28, 35, 47, 45,…\n$ cigarettes_per_week    &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n$ smoke_everyday         &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n$ past_four_weeks        &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n$ age_started_smoking    &lt;chr&gt; \"12\", \"17\", \"17\", \"16\", \"15\", \"16\", \"22\", \"11\",…\n$ country_of_origin      &lt;chr&gt; \"United Kingdom\", \"Germany\", \"Poland\", \"Austral…\n$ cpd                    &lt;chr&gt; \"6\", \"5\", \"10\", \"20\", \"10\", \"1\", \"6\", \"15\", \"12…\n$ ethnicity              &lt;chr&gt; \"White / Caucasian\", \"White / Caucasian\", \"Mixe…\n$ gender                 &lt;chr&gt; \"Female\", \"Female\", \"Male\", \"Female\", \"Female\",…\n$ last_cigarette         &lt;chr&gt; \"60\", \"780\", \"90\", \"5\", \"60\", \"720\", \"10\", \"10\"…\n$ level_education        &lt;chr&gt; \"Graduated University / College\", \"Graduated Un…\n$ technical_issues       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ FTND_1                 &lt;dbl&gt; 2, 0, 0, 3, 3, 0, 1, 2, 1, 3, 2, 2, 2, 1, 2, 0,…\n$ FTND_2                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,…\n$ FTND_3                 &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,…\n$ FTND_4                 &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,…\n$ FTND_5                 &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,…\n$ FTND_6                 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,…\n\n\n\n\n\n\nRows: 205\nColumns: 22\n$ participant_private_id &lt;dbl&gt; 631737, 631738, 631741, 631739, 631749, 631746,…\n$ consent_given          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age                    &lt;dbl&gt; 46, 54, 23, 34, 38, 19, 25, 21, 28, 35, 47, 45,…\n$ cigarettes_per_week    &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n$ smoke_everyday         &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n$ past_four_weeks        &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n$ age_started_smoking    &lt;int&gt; 12, 17, 17, 16, 15, 16, 22, 11, 18, 15, 18, 19,…\n$ country_of_origin      &lt;fct&gt; United Kingdom, Germany, Poland, Australia, Spa…\n$ cpd                    &lt;int&gt; 6, 5, 10, 20, 10, 1, 6, 15, 12, 20, 20, 15, 20,…\n$ ethnicity              &lt;fct&gt; White / Caucasian, White / Caucasian, Mixed / m…\n$ gender                 &lt;fct&gt; Female, Female, Male, Female, Female, Male, Fem…\n$ last_cigarette         &lt;dbl&gt; 60, 780, 90, 5, 60, 720, 10, 10, 30, 0, 10, 30,…\n$ level_education        &lt;fct&gt; Graduated University / College, Graduated Unive…\n$ technical_issues       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ FTND_1                 &lt;dbl&gt; 2, 0, 0, 3, 3, 0, 1, 2, 1, 3, 2, 2, 2, 1, 2, 0,…\n$ FTND_2                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,…\n$ FTND_3                 &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,…\n$ FTND_4                 &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,…\n$ FTND_5                 &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,…\n$ FTND_6                 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,…\n$ daily_smoker           &lt;fct&gt; Daily Smoker, Daily Smoker, Daily Smoker, Daily…\n$ FTND_sum               &lt;dbl&gt; 3, 0, 2, 7, 5, 3, 3, 5, 4, 6, 7, 5, 6, 3, 4, 1,…\n\n\n\n\n\n\n\n\n\n\n\nTry this\n\n\n\nBefore we give you a task list, try and switch between the raw data and the wrangled data. Make a list of all the differences you can see between the two data objects.\n\nWhat type is each variable? Has it changed from the raw data?\nDo we have any new variables? How could you create these from the variables available to you?\n\nFor the variable daily_smoker, this has two levels which you cannot see in the preview: “Daily Smoker” and “Non-daily Smoker”. Which variable could this be based on?\nTry and wrangle the data based on all the differences you notice to create a new object demog_tidy.\nWhen you get as far as you can, check the task list which explains all the steps we applied, but not how to do them. Then, you can check the solution for our code.\n\n\n\n1.3.1 Task list\n\n\n\n\n\n\nShow me the task list\n\n\n\n\n\nThese are all the steps we applied to create the wrangled data object:\n\nConvert age_started_smoking to an integer (as age is a round number).\nConvert cpd to an integer (as cigarettes per day is a round number). You will notice a warning about introducing an NA as some nonsense responses cannot be converted to a number.\nConvert country_of_origin to a factor (as we have distinct categories).\nConvert ethnicity to a factor (as we have distinct categories).\nConvert gender to a factor (as we have distinct categories).\nConvert last_cigarette to an integer (as time since last cigarette in minutes is a round number). You will notice a warning about introducing an NA as some nonsense responses cannot be converted to a number.\nConvert level_education to a factor (as we have distinct categories).\nCreate a new variable daily_smoker by recoding an existing variable. The new variable should have two levels: “Daily Smoker” and “Non-daily Smoker”. In the process, convert daily_smoker to a factor (as we have distinct categories).\nCreate a new variable FTND_sum by taking the sum of the six items FTND_1 to FTND_6 per participant.\n\nFor some advice, think of everything you have covered so far in your course book. How could you complete these steps as efficiently as possible? Could you string together functions using pipes, or do you need some intermediary objects? If it’s easier for you to complete steps with longer but accurate code, there is nothing wrong with that. You recognise ways to make your code more efficient over time.\n\n\n\n\n1.3.2 Solution\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThis is the code we used to create the new object demog_tidy using the original object demog. As long as you get the same end result, the exact code is not important. In coding, there are multiple ways of getting to the same end result. Maybe you found a more efficient way to complete some of the steps compared to us. Maybe your code was a little longer. As long as it worked, that is the most important thing.\n\n# Using demog, create a new object demog_tidy\n# apply mutate to convert or create variables\ndemog_tidy &lt;- demog %&gt;% \n  mutate(age_started_smoking = as.integer(age_started_smoking),\n         cpd = as.integer(cpd),\n         country_of_origin = as.factor(country_of_origin),\n         ethnicity = as.factor(ethnicity),\n         gender = as.factor(gender),\n         last_cigarette = as.integer(last_cigarette),\n         level_education = as.factor(level_education),\n         # we used smoke_everyday to create our daily_smoker variable\n         daily_smoker = as.factor(case_match(smoke_everyday,\n                                             \"Yes\" ~ \"Daily Smoker\",\n                                             \"No\" ~ \"Non-daily Smoker\")))\n# To calculate the sum of the 6 FTND items, \n# pivot longer, group by ID, then sum responses. \nFTND_sum &lt;- demog_tidy %&gt;% \n  pivot_longer(cols = FTND_1:FTND_6,\n               names_to = \"Item\",\n               values_to = \"Response\") %&gt;% \n  group_by(participant_private_id) %&gt;% \n  summarise(FTND_sum = sum(Response))\n\n# Join this new column back to demog_tidy\ndemog_tidy &lt;- demog_tidy %&gt;% \n  inner_join(y = FTND_sum,\n             by = \"participant_private_id\")",
    "crumbs": [
      "Wrangling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bartlett et al. (2022): Attentional bias in smokers</span>"
    ]
  },
  {
    "objectID": "Bartlett-analysis-journey-wrangle.html#wrangling-trials",
    "href": "Bartlett-analysis-journey-wrangle.html#wrangling-trials",
    "title": "1  Bartlett et al. (2022): Attentional bias in smokers",
    "section": "\n1.4 Wrangling trials",
    "text": "1.4 Wrangling trials\nTurning to the trials file, there are a few wrangling steps and you will probably need the task list more for this part than you did for demographics. Some of the steps might not be as obvious but it is still important to compare the objects and see if you can identify the changes. We are going to show you a preview of the starting data set, and the end product we are aiming for in step 3.\n\n\nOriginal raw data\nStep 1\nStep 2\nStep 3\n\n\n\n\n\nRows: 244,847\nColumns: 15\n$ participant_private_id &lt;dbl&gt; 631737, 631737, 631737, 631737, 631737, 631737,…\n$ trial_number           &lt;chr&gt; \"BEGIN TASK\", \"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\"…\n$ reaction_time          &lt;dbl&gt; NA, 8007.015, 249.823, 199.765, 1999.671, 249.8…\n$ response               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ correct                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ display                &lt;chr&gt; NA, \"instructions\", \"practice\", \"practice\", \"pr…\n$ answer                 &lt;chr&gt; NA, NA, \"right\", \"right\", \"right\", \"left\", \"lef…\n$ soa                    &lt;dbl&gt; NA, NA, 200, 200, 200, 200, 200, 200, 500, 500,…\n$ screen_name            &lt;chr&gt; NA, \"instructions_continue\", \"Screen 1\", \"Scree…\n$ image_left             &lt;chr&gt; NA, NA, \"p1.jpg\", \"p1.jpg\", \"p1.jpg\", \"p1.jpg\",…\n$ image_right            &lt;chr&gt; NA, NA, \"p2.jpg\", \"p2.jpg\", \"p2.jpg\", \"p2.jpg\",…\n$ dot_left               &lt;chr&gt; NA, NA, \"nodot.jpg\", \"nodot.jpg\", \"nodot.jpg\", …\n$ dot_right              &lt;chr&gt; NA, NA, \"dot.jpg\", \"dot.jpg\", \"dot.jpg\", \"nodot…\n$ trial_type             &lt;chr&gt; NA, NA, \"practice\", \"practice\", \"practice\", \"pr…\n$ block                  &lt;dbl&gt; NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\n\n\n\nRows: 50,510\nColumns: 15\n$ participant_private_id &lt;dbl&gt; 631737, 631737, 631737, 631737, 631737, 631737,…\n$ trial_number           &lt;chr&gt; \"1\", \"2\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"11\", \"…\n$ reaction_time          &lt;dbl&gt; 1486.850, 720.640, 739.635, 668.730, 578.015, 5…\n$ response               &lt;chr&gt; \"right\", \"left\", \"right\", \"right\", \"left\", \"lef…\n$ correct                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ display                &lt;chr&gt; \"trials\", \"trials\", \"trials\", \"trials\", \"trials…\n$ answer                 &lt;chr&gt; \"right\", \"left\", \"right\", \"right\", \"left\", \"lef…\n$ soa                    &lt;dbl&gt; 200, 500, 500, 200, 200, 200, 200, 200, 500, 50…\n$ screen_name            &lt;chr&gt; \"response\", \"response\", \"response\", \"response\",…\n$ image_left             &lt;chr&gt; \"N14.jpg\", \"F14.jpg\", \"F14.jpg\", \"N4.jpg\", \"N19…\n$ image_right            &lt;chr&gt; \"F14.jpg\", \"N14.jpg\", \"N14.jpg\", \"F4.jpg\", \"F19…\n$ dot_left               &lt;chr&gt; \"nodot.jpg\", \"dot.jpg\", \"nodot.jpg\", \"nodot.jpg…\n$ dot_right              &lt;chr&gt; \"dot.jpg\", \"nodot.jpg\", \"dot.jpg\", \"dot.jpg\", \"…\n$ trial_type             &lt;chr&gt; \"smoking\", \"smoking\", \"nonsmoking\", \"smoking\", …\n$ block                  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n\n\n\nRows: 820\nColumns: 5\n$ participant_private_id &lt;dbl&gt; 631737, 631737, 631737, 631737, 631738, 631738,…\n$ soa                    &lt;dbl&gt; 200, 200, 500, 500, 200, 200, 500, 500, 200, 20…\n$ trial_type             &lt;chr&gt; \"nonsmoking\", \"smoking\", \"nonsmoking\", \"smoking…\n$ median_RT              &lt;dbl&gt; 444.6275, 452.8675, 456.8900, 449.6650, 638.000…\n$ condition              &lt;chr&gt; \"nonsmoking200\", \"smoking200\", \"nonsmoking500\",…\n\n\n\n\n\n\nRows: 205\nColumns: 5\n$ participant_private_id &lt;dbl&gt; 631737, 631738, 631739, 631741, 631746, 631748,…\n$ nonsmoking200          &lt;dbl&gt; 444.6275, 638.0000, 516.5375, 410.8250, 375.272…\n$ smoking200             &lt;dbl&gt; 452.8675, 703.5000, 529.1300, 433.3175, 367.365…\n$ nonsmoking500          &lt;dbl&gt; 456.8900, 700.0000, 517.4775, 427.4100, 363.195…\n$ smoking500             &lt;dbl&gt; 449.6650, 725.0000, 514.5050, 421.2250, 355.777…\n\n\n\n\n\n\n\n\n\n\n\nTry this\n\n\n\nBefore we give you a task list, try and switch between the raw data and the three steps we took for wrangling the data. Make a list of all the differences you can see across the steps.\nThis part of the data wrangling is quite difficult if you are unfamiliar with dealing with response time tasks as you need to know what the end product should look like to work with later. Essentially, we want the median response time per participant per condition (across trial type and SOA). There are rows we do not need, variables to create, and data to restructure. So, it takes all your wrangling skills you have learnt so far.\n\nIn step 1, how many observations do we have compared to the raw data? Knowing the design is important here, so look at the columns correct, screen_name, and trial_type. What function might reduce the number of observations like this?\nIn step 2, how many observations do we have compared to step 1? How many observations do we have per participant ID? What new variables do we have and how could you make them? Hint: for condition, we have not covered this, so look up a function called paste0().\nIn step 3, how many observations do we have compared to step 2? Have we removed any columns compared to step 2? Have the data been restructured?\n\nTry and wrangle the data based on all the differences you notice to create a new object RT_wide shown in step 3.\nWhen you get as far as you can, check the task list which explains all the steps we applied, but not how to do them. Then, you can check the solution for our code.\n\n\n\n1.4.1 Task list\nFor this part, we will separate the task list into the three steps in case you want to test yourself at each stage.\n\n\n\n\n\n\nShow me the step 1 task list\n\n\n\n\n\nThese are all the steps we applied to create the wrangled data object:\n\nCreate an object trials_tidy using the original trials data.\n\nFilter observations using three variables:\n\nscreen_name should only include “response”.\ntrial_type should only include “nonsmoking” and “smoking”.\ncorrect should only include 1 (correct responses).\n\n\n\n\n\n\n\n\n\n\n\n\nShow me the step 2 task list\n\n\n\n\n\nThese are all the steps we applied to create the wrangled data object:\n\nCreate an object average_trials using the trials_tidy object from step 1.\nGroup observations by three variables: participant_private_id, soa, and trial_type.\nSummarise the data to create a new variable median_RT by calculating the median reaction_time.\nCreate a new variable called condition by combining the names of the trial_type and soa columns. Hint: this is maybe a new concept, so try this paste0(trial_type, soa). There are a few ways of dealing with this problem, but we are trying to avoid turning soa into variable names, as R does not like having variable names start with or be completely numbers.\nUngroup to avoid the groups carrying over into future objects.\n\n\n\n\n\n\n\n\n\n\nShow me the step 3 task list\n\n\n\n\n\nThese are all the steps we applied to create the wrangled data object:\n\nCreate an object RT_wide using the average_trials object from step 2.\nRemove the variables soa and trial_type to avoid problems with restructuring. You could use the argument,\nRestructure the data so your condition variable is spread across columns.\n\n\n\n\n\n1.4.2 Solution\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThis is the code we used to create the new object RT_wide by following three steps. You could do it in two to combine the first two steps, but we wanted to make the change between filtering and grouping/summarising more obvious before showing you the task list.\nRemember: As long as you get the same end result, the exact code is not important. In coding, there are multiple ways of getting to the same end result.\n\n# filter trials to focus on correct responses and key trials\ntrials_tidy &lt;- trials %&gt;% \n  filter(screen_name == \"response\",\n         trial_type %in% c(\"nonsmoking\", \"smoking\"),\n         correct == 1)\n\n# Calculate median RT per ID, SOA, and trial type\naverage_trials &lt;- trials_tidy %&gt;% \n  group_by(participant_private_id, soa, trial_type) %&gt;% \n  summarise(median_RT = median(reaction_time)) %&gt;% \n  mutate(condition = paste0(trial_type, soa)) %&gt;% \n  ungroup() # ungroup to avoid it carrying over\n\n# Create wide data by making a new condition variable\n# remove soa and trial type\n# pivot wider for four columns per participant\nRT_wide &lt;- average_trials %&gt;% \n  select(-soa, -trial_type) %&gt;% \n  pivot_wider(names_from = condition,\n              values_from = median_RT)",
    "crumbs": [
      "Wrangling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bartlett et al. (2022): Attentional bias in smokers</span>"
    ]
  },
  {
    "objectID": "Bartlett-analysis-journey-wrangle.html#combining-objects-and-exclusion-criteria",
    "href": "Bartlett-analysis-journey-wrangle.html#combining-objects-and-exclusion-criteria",
    "title": "1  Bartlett et al. (2022): Attentional bias in smokers",
    "section": "\n1.5 Combining objects and exclusion criteria",
    "text": "1.5 Combining objects and exclusion criteria\nGreat work so far! You should now have two wrangled objects: demog_tidy and RT_wide. The next step is combining them and applying exclusion criteria from the study.\nWe are going to give you the task list immediately for this as you need to understand the methods to know what criteria to use. We still challenge you to complete the tasks though, before checking your answers against the code we used.\n\n\n\n\n\n\nTask list\n\n\n\nComplete the following tasks to apply the final data wrangling steps:\n\nCreate a new object called full_data by joining your two data objectsdemog_tidy and RT_wide using a common identifier.\n\nFilter observations using the following criteria:\n\nconsent_given should only include 1. We only want people who consented.\nage range only between 18 and 60. We do not want people younger or older than this range.\npast_four_weeks should only include “Yes”. We do not want people who have not smoked in the past four weeks.\ntechnical_issues should only include “No”. We do not want people who experienced technical issues during the study.\n\n\n\n\n\n\n1.5.1 Solution\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThis is the code we used to create the new object full_data by joining demog_tidy and RT_wide, and filtering observations based on our four criteria.\nAs long as you get the same end result, the exact code is not important. In coding, there are multiple ways of getting to the same end result.\n\n# create full_data by joining the two objects\n# filter data by four criteria\nfull_data &lt;- demog_tidy %&gt;% \n  inner_join(y = RT_wide,\n             by = \"participant_private_id\") %&gt;% \n  filter(consent_given == 1,\n         age &gt;= 18 & age &lt;= 60,\n         past_four_weeks == \"Yes\",\n         technical_issues == \"No\")",
    "crumbs": [
      "Wrangling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bartlett et al. (2022): Attentional bias in smokers</span>"
    ]
  },
  {
    "objectID": "Bartlett-analysis-journey-wrangle.html#summarisingvisualising-your-data",
    "href": "Bartlett-analysis-journey-wrangle.html#summarisingvisualising-your-data",
    "title": "1  Bartlett et al. (2022): Attentional bias in smokers",
    "section": "\n1.6 Summarising/visualising your data",
    "text": "1.6 Summarising/visualising your data\nThat is all the wrangling complete! Hopefully, this reinforces the role of reproducibility and data skills. If you did this in other software like Excel, you might not have a paper trail of all the steps. Like this, you have the full code to apply all the wrangling steps from raw data which you can run every time you need to, and edit it if you found a mistake or wanted to add something new. You can also come back to the file later to add more code, such as creating more sophisticated plots or calculating inferential statistics.\nTo finish the journey, we have some practice tasks for summarising and visualising the data. The whole purpose of Bartlett et al. (2022) was to compare daily and non-daily smokers, so we will explore some of the key variables.\nAll of the questions are based on the final full_data object. If your answers differ, check the wrangling steps above. If you are really struggling to identify the difference, or you just wanted to complete these tasks, you can download full_data here: Bartlett_full_data.csv.\n\n1.6.1 Demographics\n\nHow many daily and non-daily smokers were there? There were  daily smokers and  non-daily smokers.\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nfull_data %&gt;% \n  count(daily_smoker)\n\n\n\n\ndaily_smoker\nn\n\n\n\nDaily Smoker\n115\n\n\nNon-daily Smoker\n63\n\n\n\n\n\n\n\n\n\n\nTo 2 decimal places, the mean age of all the participants was  (SD = ).\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nfull_data %&gt;% \n  summarise(mean_age = round(mean(age), 2),\n            sd_age = round(sd(age), 2))\n\n\n\n\nmean_age\nsd_age\n\n\n31.07\n9.22\n\n\n\n\n\n\n\n\n\nA histogram of all the participants’ ages looks like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nfull_data %&gt;% \n  ggplot(aes(x = age)) + \n  geom_histogram() + \n  scale_x_continuous(name = \"Age\") + \n  scale_y_continuous(name = \"Frequency\") + \n  theme_classic()\n\n\n\n\n\nA bar plot of the gender breakdown of the sample would look like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nfull_data %&gt;% \n  ggplot(aes(x = gender)) + \n  geom_bar() +\n  scale_x_discrete(name = \"Gender\") + \n  scale_y_continuous(name = \"Frequency\") + \n  theme_classic()\n\n\n\n\n\n1.6.2 Measures of smoking dependence\n\nTo 2 decimal places, for daily smokers the mean number of cigarettes per day was  (SD = ) and for non-daily smokers the mean number of cigarettes per day was  (SD = ).\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nfull_data %&gt;% \n  group_by(daily_smoker) %&gt;% \n  summarise(mean_cpd = round(mean(cpd, na.rm = TRUE), 2),\n            sd_cpd = round(sd(cpd, na.rm = TRUE), 2))\n\n\n\n\ndaily_smoker\nmean_cpd\nsd_cpd\n\n\n\nDaily Smoker\n8.85\n6.51\n\n\nNon-daily Smoker\n2.32\n2.69\n\n\n\n\n\n\n\n\n\n\nTo 2 decimal places, for daily smokers the mean FTND sum score was  (SD = ) and for non-daily smokers the mean number of cigarettes per day was  (SD = ).\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nfull_data %&gt;% \n  group_by(daily_smoker) %&gt;% \n  summarise(mean_FTND = round(mean(FTND_sum, na.rm = TRUE), 2),\n            sd_FTND = round(sd(FTND_sum, na.rm = TRUE), 2))\n\n\n\n\ndaily_smoker\nmean_FTND\nsd_FTND\n\n\n\nDaily Smoker\n2.61\n2.20\n\n\nNon-daily Smoker\n0.49\n1.28\n\n\n\n\n\n\n\n\n\n\n1.6.3 Attentional bias\n\n\nBefore answering the following questions, complete one extra data wrangling step to create difference scores where positive values mean attentional bias towards smoking images (faster responses to smoking compared to non-smoking stimuli):\n\nCreate a new variable called difference_200 by calculating nonsmoking200 - smoking200.\nCreate a new variable called difference_500 by by calculating nonsmoking500 - smoking500.\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nfull_data &lt;- full_data %&gt;% \n  mutate(difference_200 = nonsmoking200 - smoking200,\n         difference_500 = nonsmoking500 - smoking500)\n\n\n\n\n\nTo 2 decimal places, the mean difference in attentional bias in the 200ms condition was  (SD = ) for daily smokers and  (SD = ) for non-daily smokers.\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nfull_data %&gt;% \n  group_by(daily_smoker) %&gt;% \n  summarise(mean_bias_200 = round(mean(difference_200, na.rm = TRUE), 2),\n            sd_bias_200 = round(sd(difference_200, na.rm = TRUE), 2))\n\n\n\n\ndaily_smoker\nmean_bias_200\nsd_bias_200\n\n\n\nDaily Smoker\n1.25\n23.91\n\n\nNon-daily Smoker\n-2.74\n21.27\n\n\n\n\n\n\n\n\n\n\nTo 2 decimal places, the mean difference in attentional bias in the 500ms condition was  (SD = ) for daily smokers and  (SD = ) for non-daily smokers.\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nfull_data %&gt;% \n  group_by(daily_smoker) %&gt;% \n  summarise(mean_bias_500 = round(mean(difference_500, na.rm = TRUE), 2),\n            sd_bias_500 = round(sd(difference_500, na.rm = TRUE), 2))\n\n\n\n\ndaily_smoker\nmean_bias_500\nsd_bias_500\n\n\n\nDaily Smoker\n0.76\n21.66\n\n\nNon-daily Smoker\n-1.19\n14.51\n\n\n\n\n\n\n\n\n\nA difference score of 0 means no bias towards smoking or non-smoking images. So, you can see the paper did not find either group showed much attentional bias towards smoking images nor much difference between the groups, hence why it was published in the Journal of Trial and Error.\nDepending on the courses you have completed so far, you might notice we average over the trials and calculate difference scores which is not always optimal. If you are familiar with mixed effects models, you might want to try analysing the data at the trial level.",
    "crumbs": [
      "Wrangling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bartlett et al. (2022): Attentional bias in smokers</span>"
    ]
  },
  {
    "objectID": "Bartlett-analysis-journey-wrangle.html#conclusion",
    "href": "Bartlett-analysis-journey-wrangle.html#conclusion",
    "title": "1  Bartlett et al. (2022): Attentional bias in smokers",
    "section": "\n1.7 Conclusion",
    "text": "1.7 Conclusion\nWell done! Hopefully you recognised how far your skills have come to be able to do this independently, regardless of how many hints you needed.\nThese are real skills people use in research. If you are curious, Bartlett et al. (2022) shared their code as a reproducible manuscrupt, so you can see all the wrangling steps they completed by looking at this file on the Open Science Framework.\n\n\n\n\nBartlett, J. E., Jenks, R., & Wilson, N. (2022). No Meaningful Difference in Attentional Bias Between Daily and Non-Daily Smokers. Journal of Trial & Error. https://doi.org/10.36850/e11",
    "crumbs": [
      "Wrangling",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bartlett et al. (2022): Attentional bias in smokers</span>"
    ]
  },
  {
    "objectID": "Evans-analysis-journey-analysis.html",
    "href": "Evans-analysis-journey-analysis.html",
    "title": "2  Evans et al. (2023): Canine-assisted interventions for wellbeing",
    "section": "",
    "text": "2.1 Task preparation",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evans et al. (2023): Canine-assisted interventions for wellbeing</span>"
    ]
  },
  {
    "objectID": "Evans-analysis-journey-analysis.html#task-preparation",
    "href": "Evans-analysis-journey-analysis.html#task-preparation",
    "title": "2  Evans et al. (2023): Canine-assisted interventions for wellbeing",
    "section": "",
    "text": "2.1.1 Introduction to the data set\nFor this task, we are using open data from Binfet et al. (2022), where the authors used the data set to write a separate article on repurposing it for statistics education (Evans et al., 2023), inspiring us to use it in the chapter here. The abstract of their article is:\n\nResearchers have claimed that canine-assisted interventions (CAIs) contribute significantly to bolstering participants’ wellbeing, yet the mechanisms within interactions have received little empirical attention. The aim of this study was to assess the impact of client–canine contact on wellbeing outcomes in a sample of 284 undergraduate college students (77% female; 21% male, 2% non-binary). Participants self-selected to participate and were randomly assigned to one of two canine interaction treatment conditions (touch or no touch) or to a handler-only condition with no therapy dog present. To assess self-reports of wellbeing, measures of flourishing, positive and negative affect, social connectedness, happiness, integration into the campus community, stress, homesickness, and loneliness were administered. Exploratory analyses were conducted to assess whether these wellbeing measures could be considered as measuring a unidimensional construct. This included both reliability analysis and exploratory factor analysis. Based on the results of these analyses we created a composite measure using participant scores on a latent factor. We then conducted the tests of the four hypotheses using these factor scores. Results indicate that participants across all conditions experienced enhanced wellbeing on several measures; however, only those in the direct contact condition reported significant improvements on all measures of wellbeing. Additionally, direct interactions with therapy dogs through touch elicited greater wellbeing benefits than did no touch/indirect interactions or interactions with only a dog handler. Similarly, analyses using scores on the wellbeing factor indicated significant improvement in wellbeing across all conditions (handler-only, d=0.18, p=0.041; indirect, d=0.38, p&lt;0.001; direct, d=0.78, p&lt;0.001), with more benefit when a dog was present (d=0.20, p&lt;0.001), and the most benefit coming from physical contact with the dog (d=0.13, p=0.002). The findings hold implications for post-secondary wellbeing programs as well as the organization and delivery of CAIs.\n\nIn summary, they were interested in the effect of therapy dogs on well-being in undergraduate students. Participants were randomly allocated to one of three groups:\n\nCanine interaction touching the dogs (Direct).\nCanine interaction not touching the dogs (Indirect).\nHandler-only with no dogs present (Control).\n\nThey measured 9 outcomes before and after the intervention including social connectedness, stress, and loneliness. For this journey chapter, we will focus on a constrained set of variables and analyses so it does not take forever, but the process would apply to all the outcomes. The authors posed three hypotheses which we will test after some data wrangling:\n\nAll treatment groups would have significantly higher measures of well-being and lower measures of ill-being after treatment.\nThe treatment groups that interact with dogs would have significantly higher measures of well-being and lower measures of ill-being compared to the handler-only treatment.\nDirect contact with a therapy dog would yield greater benefits than indirect contact treatment.\n\n2.1.2 Organising your files and project for the task\nBefore we can get started, you need to organise your files and project for the task, so your working directory is in order.\n\nYou can create a folder for the data analysis journeys book and a new sub-folder for this chapter, something like: analysis_journeys, create a new folder for this chapter called something like Evans_analysis. Within Evans_analysis, create two new folders called data and figures.\nCreate an R Project for Evans_analysis as an existing directory for your chapter folder. This should now be your working directory.\nCreate a new R Markdown or Quarto document and give it a sensible title describing the chapter, such as Evans et al. (2023): Simple Linear Regression. Delete everything below line 10 so you have a blank file to work with and save the file in your Evans_analysis folder.\nWe are working with a new data set, so please save the following data file: Evans_2023_raw.csv. Right click the link and select “save link as”, or clicking the link will save the files to your Downloads. Make sure that you save the file as “.csv”. Save or copy the file to your data/ folder within Evans_analysis.\n\nYou are now ready to start working on the task!",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evans et al. (2023): Canine-assisted interventions for wellbeing</span>"
    ]
  },
  {
    "objectID": "Evans-analysis-journey-analysis.html#overview",
    "href": "Evans-analysis-journey-analysis.html#overview",
    "title": "2  Evans et al. (2023): Canine-assisted interventions for wellbeing",
    "section": "\n2.2 Overview",
    "text": "2.2 Overview\n\n2.2.1 Load tidyverse and read the data file\nBefore we explore what wrangling we need to do, complete the following task list and check the solution if you are stuck.\n\n\n\n\n\n\nTry this\n\n\n\nComplete the following steps:\n\nLoad the tidyverse package.\nRead the data file data/Evans_2023_raw.csv to the object name evans_data.\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nYou should have the following in a code chunk:\n\n# load the relevant packages\nlibrary(tidyverse)\n\n# Read the Evans_2023_raw.csv file \nevans_data &lt;- read_csv(\"data/Evans_2023_raw.csv\")\n\n\n\n\n\n2.2.2 Explore evans_data\n\nIn evans_data, we have the participant ID (RID), several demographic variables, and pre- and post-test items for stress, loneliness, and social connectedness. There are 88 variables which would take up loads of space, so we are just showing a preview of the first 20 here. If you use glimpse(), you will see all 88.\n\n\nRows: 284\nColumns: 20\n$ RID             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ GroupAssignment &lt;chr&gt; \"Control\", \"Direct\", \"Indirect\", \"Control\", \"Direct\", …\n$ Age_Yrs         &lt;dbl&gt; 21, 19, 18, 18, 19, 20, 26, 17, 21, 22, 19, 20, 19, 19…\n$ Year_of_Study   &lt;dbl&gt; 3, 1, 1, 1, 1, 2, 2, 1, 3, 4, 2, 2, 2, 2, 3, 1, 1, 1, …\n$ Live_Pets       &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, NA, 2, 2, 1,…\n$ Consumer_BARK   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, …\n$ S1_1            &lt;dbl&gt; 2, 2, 4, 2, 3, 4, 4, 3, 2, 2, 3, 2, 3, 4, 3, 2, 4, 2, …\n$ L1_1            &lt;dbl&gt; 3, 3, 3, 4, 2, 4, 3, 2, 3, 4, 3, 3, 4, 4, 4, 2, 3, 4, …\n$ L1_2            &lt;dbl&gt; 3, 2, 3, 2, 3, 3, 2, 3, 4, 1, 3, 3, 2, 3, 1, 4, 3, 2, …\n$ L1_3            &lt;dbl&gt; 4, 3, 2, 2, 3, 3, 1, 3, 3, 1, 2, 2, 1, 3, 1, 3, 3, 1, …\n$ L1_4            &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 4, 1, 2, 3, 2, 3, 2, 4, 3, 2, …\n$ L1_5            &lt;dbl&gt; 2, 4, 3, 4, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, …\n$ L1_6            &lt;dbl&gt; 3, 3, 4, 4, 3, 2, 3, 3, 3, 4, 3, 3, 4, 3, 4, 2, 2, 4, …\n$ L1_7            &lt;dbl&gt; 1, 2, 2, 1, 2, 2, 4, 2, 2, 1, 3, 2, 3, 2, 2, 2, 3, 3, …\n$ L1_8            &lt;dbl&gt; 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 4, 3, 2, …\n$ L1_9            &lt;dbl&gt; 3, 4, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 4, 4, 4, 3, 2, 3, …\n$ L1_10           &lt;dbl&gt; 4, 3, 3, 4, 2, 2, 3, 3, 3, 4, 4, 3, 4, 4, 4, 2, 2, 3, …\n$ L1_11           &lt;dbl&gt; 3, 2, 2, 2, 4, 3, 4, 2, 2, 1, 3, 2, 2, 2, 2, 4, 3, 2, …\n$ L1_12           &lt;dbl&gt; 1, 2, 2, 1, 4, 3, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3, 1, …\n$ L1_13           &lt;dbl&gt; 3, 1, 2, 2, 4, 3, 3, 3, 4, 1, 3, 4, 2, 2, 2, 4, 3, 2, …\n\n\nThe columns (variables) we have in the data set are:\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\nRID\ndouble\nParticipant ID number.\n\n\nGroupAssignment\ncharacter\nRandomly allocated study group: Control, Indirect, Direct.\n\n\nAge_Yrs\ndouble\nAge in years.\n\n\nYear_of_Study\ndouble\nParticipant’s year in college: First (1), Second (2), Third (3), Fourth (4), Fifth or more (5).\n\n\nLive_Pets\ndouble\nDoes the participant have a pet back at home: Pet back home (1), no pet back home (2).\n\n\nConsumer_BARK\ndouble\nIs the participant a low (1), medium (2), or high (3) consumer of the BARK program - the therapy dog service.\n\n\nS1_1\ndouble\nStress scale pre-test, 1 item, 1 (not at all stressed) to 5 (very stressed).\n\n\nL1_1 to L1_20\ndouble\nLoneliness scale pre-test, 20 items, 1 (never) to 4 (often).\n\n\nSC1_1 to SC1_20\ndouble\nSocial connectedness scale pre-test, 20 items, 1 (strongly disagree) to 6 (strongly agree).\n\n\nS2_1\ndouble\nStress scale post-test, 1 item.\n\n\nL2_1 to L2_20\ndouble\nLoneliness scale post-test, 20 items.\n\n\nSC2_1 to SC2_20\ndouble\nSocial connectedness scale post-test, 20 items, 1 (strongly disagree) to 6 (strongly agree).\n\n\n\n\n\n\n\n\n\nTry this\n\n\n\nNow we have introduced the data set, explore them using different methods we introduced. For example, opening the data object as a tab to scroll around, explore with glimpse(), or even try plotting some of the individual variables to see what they look like.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evans et al. (2023): Canine-assisted interventions for wellbeing</span>"
    ]
  },
  {
    "objectID": "Evans-analysis-journey-analysis.html#wrangling",
    "href": "Evans-analysis-journey-analysis.html#wrangling",
    "title": "2  Evans et al. (2023): Canine-assisted interventions for wellbeing",
    "section": "\n2.3 Wrangling",
    "text": "2.3 Wrangling\nWe are going to show you a preview of the starting data set and the end product we are aiming for. For the raw data, we have limited this to the first 20 rows again just so it does not take up the whole page, but if you use glimpse() you will see all 88 variables.\n\n\nRaw data\nWrangled data\n\n\n\n\n\nRows: 284\nColumns: 20\n$ RID             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ GroupAssignment &lt;chr&gt; \"Control\", \"Direct\", \"Indirect\", \"Control\", \"Direct\", …\n$ Age_Yrs         &lt;dbl&gt; 21, 19, 18, 18, 19, 20, 26, 17, 21, 22, 19, 20, 19, 19…\n$ Year_of_Study   &lt;dbl&gt; 3, 1, 1, 1, 1, 2, 2, 1, 3, 4, 2, 2, 2, 2, 3, 1, 1, 1, …\n$ Live_Pets       &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, NA, 2, 2, 1,…\n$ Consumer_BARK   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, …\n$ S1_1            &lt;dbl&gt; 2, 2, 4, 2, 3, 4, 4, 3, 2, 2, 3, 2, 3, 4, 3, 2, 4, 2, …\n$ L1_1            &lt;dbl&gt; 3, 3, 3, 4, 2, 4, 3, 2, 3, 4, 3, 3, 4, 4, 4, 2, 3, 4, …\n$ L1_2            &lt;dbl&gt; 3, 2, 3, 2, 3, 3, 2, 3, 4, 1, 3, 3, 2, 3, 1, 4, 3, 2, …\n$ L1_3            &lt;dbl&gt; 4, 3, 2, 2, 3, 3, 1, 3, 3, 1, 2, 2, 1, 3, 1, 3, 3, 1, …\n$ L1_4            &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 4, 1, 2, 3, 2, 3, 2, 4, 3, 2, …\n$ L1_5            &lt;dbl&gt; 2, 4, 3, 4, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, …\n$ L1_6            &lt;dbl&gt; 3, 3, 4, 4, 3, 2, 3, 3, 3, 4, 3, 3, 4, 3, 4, 2, 2, 4, …\n$ L1_7            &lt;dbl&gt; 1, 2, 2, 1, 2, 2, 4, 2, 2, 1, 3, 2, 3, 2, 2, 2, 3, 3, …\n$ L1_8            &lt;dbl&gt; 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 4, 3, 2, …\n$ L1_9            &lt;dbl&gt; 3, 4, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 4, 4, 4, 3, 2, 3, …\n$ L1_10           &lt;dbl&gt; 4, 3, 3, 4, 2, 2, 3, 3, 3, 4, 4, 3, 4, 4, 4, 2, 2, 3, …\n$ L1_11           &lt;dbl&gt; 3, 2, 2, 2, 4, 3, 4, 2, 2, 1, 3, 2, 2, 2, 2, 4, 3, 2, …\n$ L1_12           &lt;dbl&gt; 1, 2, 2, 1, 4, 3, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3, 1, …\n$ L1_13           &lt;dbl&gt; 3, 1, 2, 2, 4, 3, 3, 3, 4, 1, 3, 4, 2, 2, 2, 4, 3, 2, …\n\n\n\n\n\n\nRows: 284\nColumns: 12\n$ RID             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ GroupAssignment &lt;chr&gt; \"Control\", \"Direct\", \"Indirect\", \"Control\", \"Direct\", …\n$ Age_Yrs         &lt;dbl&gt; 21, 19, 18, 18, 19, 20, 26, 17, 21, 22, 19, 20, 19, 19…\n$ Year_of_Study   &lt;dbl&gt; 3, 1, 1, 1, 1, 2, 2, 1, 3, 4, 2, 2, 2, 2, 3, 1, 1, 1, …\n$ Live_Pets       &lt;chr&gt; \"Does not have a pet back home\", \"Does not have a pet …\n$ Consumer_BARK   &lt;chr&gt; \"Low\", \"Low\", \"Low\", \"Low\", \"Low\", \"Low\", \"Low\", \"Low\"…\n$ stress_pre      &lt;dbl&gt; 2, 2, 4, 2, 3, 4, 4, 3, 2, 2, 3, 2, 3, 4, 3, 2, 4, 2, …\n$ stress_post     &lt;dbl&gt; 2, 1, 3, 2, 4, 4, 3, 2, 2, 1, 2, 2, 1, 2, 4, 2, 2, 1, …\n$ lonely_pre      &lt;dbl&gt; 2.25, 1.90, 2.25, 1.75, 2.85, 2.70, 2.40, 2.25, 2.55, …\n$ lonely_post     &lt;dbl&gt; 1.70, 1.60, 2.25, 2.05, 2.70, 2.40, 2.25, 2.00, 2.55, …\n$ social_pre      &lt;dbl&gt; 3.90, 5.15, 4.10, 4.65, 3.65, 4.35, 4.75, 4.60, 4.20, …\n$ social_post     &lt;dbl&gt; 3.800000, 5.263158, 4.150000, 5.100000, 3.600000, 4.65…\n\n\n\n\n\n\n\n\n\n\n\nTry this\n\n\n\nBefore we give you a task list, try and switch between the raw data and the wrangled data. Make a list of all the differences you can see between the two data objects.\n\nDo the values of variables change from numbers? How might you recode them using the code book above?\nLooking at the codebook, are some variables the same but renamed?\nLooking at the codebook, have we calculated the mean of all the items for a scale?\n\nTry and wrangle the data based on all the differences you notice to create a new object evans_wide.\nFor one hint, unless you read the original paper, there are a bunch of items that first need reverse coding you would not know about:\n\nLoneliness pre-test: L1_1, L1_5, L1_6, L1_9, L1_10, L1_15, L1_16, L1_19, L1_20.\nLoneliness post-test: L2_1, L2_5, L2_6, L2_9, L2_10, L2_15, L2_16, L2_19, L2_20.\nSocial connectedness pre-test: SC1_3, SC1_6, SC1_7, SC1_9, SC1_11, SC1_13, SC1_15, SC1_17, SC1_18, SC1_20.\nSocial connectedness post-test: SC2_3, SC2_6, SC2_7, SC2_9, SC2_11, SC2_13, SC2_15, SC2_17, SC2_18, SC2_20.\n\nWhen you get as far as you can, check the task list which explains all the steps we applied, but not how to do them. Then, you can check the solution for our code.\n\n\n\n2.3.1 Task list\n\n\n\n\n\n\nShow me the task list\n\n\n\n\n\nThese are all the steps we applied to create the wrangled data object:\n\nRecode Live_Pets to the two labels outlined in the code book.\nRecode Consumer_BARK to the three labels outlined in the code book.\nReverse code the loneliness and social connectedness items outlined above. Think of previous examples where we explained reverse coding for how you can do this efficiently.\n\nAs one extra piece of advice if you do not want to recode 40 variables one by one, there is a more advanced function you can use within mutate(). The function across() lets you apply a function or calculation to several columns at once. For example, if we wanted to reverse score items on a 4-point scale, it would look like the following:\n\nmutate(across(.cols = c(column1, column2...), \n              .fns = ~ 5 - .x))\n\nIn .cols, we enter all the columns we want to apply the function to.\nIn .fns after the =, we add the function we want to apply to all the columns we selected. The code is a little awkward as we have a tilde ~, here the calculation we want to apply, and .x in place of the column name. You could summarise it as: for all the columns I select, subtract each value from 5. Once you get used to the format, across() is really helpful when you want to do the same thing to multiple columns.\n\nAfter reverse coding the items, calculate the subscale mean scores for loneliness and social connectedness. You must do this twice per scale, as we have the 20 items for the pre-test and 20 items for the post-test per scale.\nIf you calculated the subscale mean scores individually, join them back to the evans_clean object you mutated.\n\nSelect the following columns:\n\nRID to Consumer_BARK.\nRename S1_1 to stress_pre.\nRename S2_1 to stress_post.\nSelect your four subscale mean score variables.\n\n\n\nRemember: If it’s easier for you to complete steps with longer but accurate code, there is nothing wrong with that. You recognise ways to make your code more efficient over time.\n\n\n\n\n2.3.2 Solution\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThis is the code we used to create the new object evans_wide using the original object evans_data. As long as you get the same end result, the exact code is not important. In coding, there are multiple ways of getting to the same end result. Maybe you found a more efficient way to complete some of the steps compared to us. Maybe your code was a little longer. As long as it worked, that is the most important thing.\n\n# Initial cleaning step to recode pets and BARK\n# then reverse code a bunch of items \nevans_clean &lt;- evans_data %&gt;% \n  mutate(Live_Pets = case_match(Live_Pets,\n                                1 ~ \"Has a pet back home\",\n                                2 ~ \"Does not have a pet back home\"),\n         Consumer_BARK = case_match(Consumer_BARK,\n                                    1 ~ \"Low\",\n                                    2 ~ \"Medium\",\n                                    3 ~ \"High\"),\n         # across works with mutate to apply the same function to several columns\n         # So, take all the loneliness items to reverse code, then subtract them from 5\n         across(.cols = c(L1_1, L1_5, L1_6, L1_9, L1_10, L1_15, L1_16, L1_19, L1_20,\n                          L2_1, L2_5, L2_6, L2_9, L2_10, L2_15, L2_16, L2_19, L2_20),\n                .fns = ~ 5 - .x),\n         # take all the connectedness items to reverse code, then subtract them from 7\n         across(.cols = c(SC1_3, SC1_6, SC1_7, SC1_9, SC1_11, SC1_13, SC1_15, SC1_17, SC1_18, SC1_20, \n                          SC2_3, SC2_6, SC2_7, SC2_9, SC2_11, SC2_13, SC2_15, SC2_17, SC2_18, SC2_20),\n                .fns = ~ 7 - .x))\n\n# There are more elegant ways around this, but for each set, \n# take the 20 items, group by participant ID, and calculate the mean, ignoring missing values\nlonely_pre &lt;- evans_clean %&gt;% \n  pivot_longer(cols = L1_1:L1_20, \n               names_to = \"Item\", \n               values_to = \"Response\") %&gt;% \n  group_by(RID) %&gt;% \n  summarise(lonely_pre = mean(Response, na.rm = TRUE))\n\n# Same thing for post scores\nlonely_post &lt;- evans_clean %&gt;% \n  pivot_longer(cols = L2_1:L2_20, \n               names_to = \"Item\", \n               values_to = \"Response\") %&gt;% \n  group_by(RID) %&gt;% \n  summarise(lonely_post = mean(Response, na.rm = TRUE))\n\n# take the 20 items, group by participant ID, and calculate the mean, ignoring missing values\nsocial_pre &lt;- evans_clean %&gt;% \n  pivot_longer(cols = SC1_1:SC1_20, \n               names_to = \"Item\", \n               values_to = \"Response\") %&gt;% \n  group_by(RID) %&gt;% \n  summarise(social_pre = mean(Response, na.rm = TRUE))\n\n# Same thing for post scores\nsocial_post &lt;- evans_clean %&gt;% \n  pivot_longer(cols = SC2_1:SC2_20, \n               names_to = \"Item\", \n               values_to = \"Response\") %&gt;% \n  group_by(RID) %&gt;% \n  summarise(social_post = mean(Response, na.rm = TRUE))\n\n# join all four summary values to main data\n# select just the key variables we need\n# rename the two stress items\nevans_wide &lt;- evans_clean %&gt;% \n  inner_join(lonely_pre) %&gt;% \n  inner_join(lonely_post) %&gt;% \n  inner_join(social_pre) %&gt;% \n  inner_join(social_post) %&gt;% \n  select(RID:Consumer_BARK, \n         stress_pre = S1_1, \n         stress_post = S2_1, \n         lonely_pre:social_post)",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evans et al. (2023): Canine-assisted interventions for wellbeing</span>"
    ]
  },
  {
    "objectID": "Evans-analysis-journey-analysis.html#summarisingvisualising",
    "href": "Evans-analysis-journey-analysis.html#summarisingvisualising",
    "title": "2  Evans et al. (2023): Canine-assisted interventions for wellbeing",
    "section": "\n2.4 Summarising/visualising",
    "text": "2.4 Summarising/visualising\nYou should now have an object called evans_wide containing 12 variables. If you struggled completing the wrangling steps, you can copy the code from the solution to follow along from this point. In this section, we will calculate some summary statistics and plot the data to see what we can learn. We present you with a list of questions to answer using your wrangling and visualisation skills, interspersed with the solutions to check if you are stuck.\n\n2.4.1 Demographics\nFor demographics, we will recreate some values from Table 1 from Binfet et al. (2022).\n\n\nHow many participants were in each group for GroupAssignment?\n\n Control\n Direct\n Indirect\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThis nicely reproduces their values.\n\nevans_wide %&gt;% \n  count(GroupAssignment)\n\n\n\n\nGroupAssignment\nn\n\n\n\nControl\n94\n\n\nDirect\n95\n\n\nIndirect\n95\n\n\n\n\n\n\n\n\n\n\n\nTo 2 decimals, what was the mean and standard deviation age per group?\n\nControl: M = , SD = .\nDirect: M = , SD = .\nIndirect: M = , SD = .\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nWeirdly, this reproduces the standard deviations from Table 1, but not the means…\n\nevans_wide %&gt;% \n  group_by(GroupAssignment) %&gt;% \n  summarise(mean_age = round(mean(Age_Yrs), 2),\n            sd_age = round(sd(Age_Yrs), 2))\n\n\n\n\nGroupAssignment\nmean_age\nsd_age\n\n\n\nControl\n19.95\n2.89\n\n\nDirect\n19.77\n1.94\n\n\nIndirect\n19.95\n2.23\n\n\n\n\n\n\n\n\n\n\n\nHow many participants in each group have a pet at home?\n\n Control\n Direct\n Indirect\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThis nicely reproduces their values.\n\nevans_wide %&gt;% \n  count(GroupAssignment, Live_Pets)\n\n\n\n\nGroupAssignment\nLive_Pets\nn\n\n\n\nControl\nDoes not have a pet back home\n72\n\n\nControl\nHas a pet back home\n21\n\n\nControl\nNA\n1\n\n\nDirect\nDoes not have a pet back home\n63\n\n\nDirect\nHas a pet back home\n30\n\n\nDirect\nNA\n2\n\n\nIndirect\nDoes not have a pet back home\n60\n\n\nIndirect\nHas a pet back home\n34\n\n\nIndirect\nNA\n1\n\n\n\n\n\n\n\n\n\n\nThis is not part of their article, but one interesting question might be how many people have a pet at home (Live_Pets) against how frequently they use the BARK program (Consumer_BARK). Try and recreate the following bar plot to visualise this as close as possible. We have intentionally used some features you may not have covered in the data visualisation chapters to get you problem solving. For one hint though, we used option D for the viridis colour scheme.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe new features we hoped you found independently were:\n\nSetting the factor order to show Consumer_BARK as low, medium, then high.\nSet position = \"dodge\" to avoid the stacked bar chart.\nEdited the legend title by using the name argument in the scale_fill layer.\n\n\nevans_wide %&gt;% \n  drop_na(Live_Pets, Consumer_BARK) %&gt;% \n  mutate(Consumer_BARK = factor(Consumer_BARK, \n                                levels = c(\"Low\", \"Medium\", \"High\"))) %&gt;% \n  ggplot(aes(x = Live_Pets, fill = Consumer_BARK)) + \n  geom_bar(position = \"dodge\") + \n  labs(x = \"Pets in Home\", \n       y = \"Frequency\") + \n  scale_fill_viridis_d(option = \"D\", \n                       name = \"BARK Program User\") + \n  theme_classic()\n\n\n\n\n\n2.4.2 Wellbeing measures\nFor wellbeing and ill-being measures, we will recreate some values from Table 2 from Binfet et al. (2022).\n\n\nIf you calculate the mean and standard deviation of each variable per group, answer the following questions:\n\nWhich group has the lowest post-test stress value? \nDirect\nControl\nIndirect\nWhich group has the lowest post-test loneliness value? \nControl\nDirect\nIndirect\nWhich group has the lowest post-test social connectedness value? \nDirect\nIndirect\nControl\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThis table calculates the mean and standard score for each variable per group. You can use this to answer the questions above.\n\nevans_wide %&gt;% \n  pivot_longer(cols = stress_pre:social_post, \n               names_to = \"Variable\", \n               values_to = \"Value\") %&gt;% \n  group_by(GroupAssignment, Variable) %&gt;% \n  summarise(mean_score = round(mean(Value), 2), \n            sd_score = round(sd(Value), 2))\n\n`summarise()` has grouped output by 'GroupAssignment'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nGroupAssignment\nVariable\nmean_score\nsd_score\n\n\n\nControl\nlonely_post\n1.96\n0.57\n\n\nControl\nlonely_pre\n2.02\n0.55\n\n\nControl\nsocial_post\n4.49\n0.87\n\n\nControl\nsocial_pre\n4.47\n0.81\n\n\nControl\nstress_post\n2.76\n1.08\n\n\nControl\nstress_pre\n3.27\n1.04\n\n\nDirect\nlonely_post\n1.82\n0.51\n\n\nDirect\nlonely_pre\n2.05\n0.56\n\n\nDirect\nsocial_post\n4.64\n0.79\n\n\nDirect\nsocial_pre\n4.42\n0.88\n\n\nDirect\nstress_post\n1.84\n0.76\n\n\nDirect\nstress_pre\n3.15\n0.98\n\n\nIndirect\nlonely_post\n1.96\n0.52\n\n\nIndirect\nlonely_pre\n2.06\n0.48\n\n\nIndirect\nsocial_post\n4.50\n0.82\n\n\nIndirect\nsocial_pre\n4.37\n0.79\n\n\nIndirect\nstress_post\n2.53\n1.00\n\n\nIndirect\nstress_pre\n3.21\n0.90\n\n\n\n\n\n\n\n\n\n\nCreate a scatterplot of the relationship between post-test social connectedness and loneliness. The relationship between the two variables is \npositive\nnegative, meaning that as social connectedness increases, we expect loneliness to \ndecrease\nincrease.\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe code here creates a scatterplot for the relationship between post-test social connectedness and loneliness.\n\nevans_wide %&gt;% \n  ggplot(aes(x = lonely_post, y = social_post)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  theme_classic() + \n  labs(x = \"Loneliness Post-test\", y = \"Social Connectedness Post-test\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nAs we start to think about inferential statistics, we can plot the difference between pre- and post-test for each group. For this question, try and recreate the boxplot to visualise stress. Hint: think about if you need to restructure the data, and how you can present the conditions in the appropriate order.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThere are two key steps before you can plot the data. First, you need to restructure the data to long form to get one variable for pre- and post-test. Second, post comes before pre due to alphabetical order, so you need to create a factor to specify the order here.\n\nevans_wide %&gt;% \n  pivot_longer(cols = stress_pre:stress_post, \n               names_to = \"Stress\", \n               values_to = \"Value\") %&gt;% \n  mutate(Stress = factor(Stress,\n                         levels = c(\"stress_pre\", \"stress_post\"), \n                         labels = c(\"Pre-test\", \"Post-test\"))) %&gt;% \n  ggplot(aes(x = GroupAssignment, y = Value, fill = Stress)) + \n  geom_boxplot(alpha = 0.7) + \n  labs(x = \"Group Assignment\", y = \"Stress Scale\") + \n  scale_fill_viridis_d(option = \"E\", \n                       name = \"Time\") + \n  theme_classic()\n\n\n\n\n\nTry and recreate the violin-boxplot to visualise loneliness. Hint: remember how to align the different elements.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe same hints apply as question 7 as you need to restructure the data and create a new factor order so pre comes before post. As we have two grouping variables, we must specify a constant position dodge value so it does not plot weird.\n\n# specify as an object, so we only change it in one place\ndodge_value &lt;- 0.9\n\nevans_wide %&gt;% \n  pivot_longer(cols = lonely_pre:lonely_post, \n               names_to = \"Lonely\", \n               values_to = \"Value\") %&gt;% \n   mutate(Lonely = factor(Lonely,\n                          levels = c(\"lonely_pre\", \"lonely_post\"), \n                          labels = c(\"Pre-test\", \"Post-test\"))) %&gt;% \n  ggplot(aes(x = GroupAssignment, y = Value, fill = Lonely)) + \n  geom_violin(alpha = 0.5) + \n  geom_boxplot(width = 0.2, \n               alpha = 0.7,\n               fatten = NULL,\n               position = position_dodge(dodge_value)) + \n  stat_summary(fun = \"mean\", \n               geom = \"point\",\n               position = position_dodge(dodge_value)) +\n  stat_summary(fun.data = \"mean_cl_boot\", \n               geom = \"errorbar\", \n               width = .1,\n               position = position_dodge(dodge_value)) +\n  scale_fill_viridis_d(option = \"E\", \n                       name = \"Time\") + \n  labs(x = \"Group Assignment\", y = \"Loneliness Scale\") + \n  theme_classic()",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evans et al. (2023): Canine-assisted interventions for wellbeing</span>"
    ]
  },
  {
    "objectID": "Evans-analysis-journey-analysis.html#analysing",
    "href": "Evans-analysis-journey-analysis.html#analysing",
    "title": "2  Evans et al. (2023): Canine-assisted interventions for wellbeing",
    "section": "\n2.5 Analysing",
    "text": "2.5 Analysing\nFinally, we turn to analysis for inferential statistics. In Evans et al. (2023), they organise the analyses from Binfet et al. (2022) into three hypotheses. You may not have developed all the analysis skills yet to reproduce their analyses exactly, so we will work around an adapted set.\nWe present each analysis as an overview so you can think about what techniques would address it, give you instructions on what analysis we have in mind if you need guidance, then present the solution. We focus on one outcome per hypothesis, but once you are confident you are applying and interpreting the appropriate techniques, why not try the other outcomes yourself?\n\n2.5.1 Hypothesis 1\n\nHypothesis 1 is that each treatment group will increase measures of well-being (social connectedness) and decrease measures of ill-being (stress and loneliness). For this question, we focus on stress, so we expected stress to decrease.\n\n\n\n\n\n\n\nTry this\n\n\n\nThere are two ways you could approach this. Either as the whole sample, or like Evans et al. (2023) present to focus on each group separately. Think about what techniques would let you test the hypothesis that each treatment group will decrease stress at post-test compared to pre-test.\n\n\n\n\n\n\n\n\nShow me the task list\n\n\n\n\n\nFor the solution below, we are focusing on the whole sample to test whether everyone decreased stress in post-test, but you could approach it by separating the data into the three groups and then testing for the difference per group.\nTo test whether an outcome decreases between conditions in the same participants, we need a model suitable for a within-subjects design. You might have covered how to test the difference in conditions either through a paired samples-test or linear model with fixed intercept on the difference score.\n\nCalculate the difference between stress pre- and post-test.\nFit a linear model on the difference score with a fixed intercept and no predictor.\nLook at the intercept, is it positive or negative? If you calculated pre-test minus post-test, you would be looking for a positive difference as we expect lower stress at post-test. For hypothesis testing, is the p-value lower than alpha?\nWhat is the effect size? How much did stress change from pre-test to post-test? What is the confidence interval around the effect size?\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nIn the code below, we first calculate a difference score by subtracting stress post-test from stress pre-test.\nWe then fit a linear model on this difference score and add a fixed intercept as we have no predictor.\nConsistent with hypothesis 1, stress was lower at post-test compared to pre-test across all participants. On average, participants decreased their stress score by 0.83 points (\\(b_0\\) = 0.83, 95% CI = [0.72, 0.95], p &lt; .001).\n\nevans_wide &lt;- evans_wide %&gt;% \n  mutate(stress_diff = stress_pre - stress_post)\n\nlm_stress &lt;- lm(stress_diff ~ 1, \n                data = evans_wide)\n\nsummary(lm_stress)\n\nconfint(lm_stress)\n\n\nCall:\nlm(formula = stress_diff ~ 1, data = evans_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8345 -0.8345  0.1655  0.1655  3.1655 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.83451    0.05657   14.75   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9534 on 283 degrees of freedom\n\n                2.5 %    97.5 %\n(Intercept) 0.7231517 0.9458623\n\n\n\n\n\n\n2.5.2 Hypothesis 2\n\nIn Hypothesis 2, we predict that the direct and indirect contact groups will have higher well-being and lower ill-being measures compared to the control group. For this question, we focus on social connectedness at post-test for a measure of well-being, so we expect higher scores in the direct and indirect groups.\n\n\n\n\n\n\n\nTry this\n\n\n\nThink about what techniques would let you test the hypothesis that the direct and indirect groups both increase social connectedness at post-test compared to the control group. Think of how you could focus on two groups at a time per analysis.\n\n\n\n\n\n\n\n\nShow me the task list\n\n\n\n\n\nFor the solution below, we apply two tests. We focus on the direct and control groups by filtering out indirect, then we focus on indirect and control by filtering out direct.\nTo test the difference between two groups on one outcome, we need a model suitable for a between-subjects design. You might have covered linear regression with one categorical predictor for how to test the difference in an outcome between two groups.\n\nFit a linear model on the outcome social connectedness post-test with a categorical predictor of group. You will need to fit two models, one where you filter out the indirect group and one where you filter out the direct group.\nLook at the intercept, what is the predicted value for the reference group? Look at the slope, what is the estimated change to the target group? For hypothesis 2, is the direct/indirect group higher on social connectedness compared to control? For hypothesis testing, is the p-value lower than alpha?\nWhat is the effect size? How much did the two groups differ on social connectedness? What is the confidence interval around the effect size?\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nIn the code below, we first fit a linear model on social connectedness post-test and focus on the direct and control groups. There are different ways you could ignore the indirect group, but here we filter the data as we pass the data to the lm() function.\nThe direct group increased social connectedness post-test on average 0.15 points compared to the control group, but the difference was not statistically significant (\\(b_1\\) = 0.15, 95% CI = [-0.09, 0.39], p = .207).\n\nlm_direct &lt;- lm(social_post ~ GroupAssignment, \n                 data = filter(evans_wide,\n                               GroupAssignment != \"Indirect\"))\n\nsummary(lm_direct)\n\nconfint(lm_direct)\n\n\nCall:\nlm(formula = social_post ~ GroupAssignment, data = filter(evans_wide, \n    GroupAssignment != \"Indirect\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2940 -0.5905  0.1338  0.6560  1.3560 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            4.49054    0.08596  52.239   &lt;2e-16 ***\nGroupAssignmentDirect  0.15349    0.12125   1.266    0.207    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8334 on 187 degrees of freedom\nMultiple R-squared:  0.008497,  Adjusted R-squared:  0.003195 \nF-statistic: 1.603 on 1 and 187 DF,  p-value: 0.2071\n\n                            2.5 %    97.5 %\n(Intercept)            4.32096331 4.6601179\nGroupAssignmentDirect -0.08569829 0.3926749\n\n\nThe indirect group increased social connectedness post-test compared to the control group, but the difference was very small and not statistically significant (\\(b_1\\) = 0.01, 95% CI = [-0.23, 0.25], p = .936).\n\nlm_indirect &lt;- lm(social_post ~ GroupAssignment, \n                 data = filter(evans_wide,\n                               GroupAssignment != \"Direct\"))\n\nsummary(lm_indirect)\n\nconfint(lm_indirect)\n\n\nCall:\nlm(formula = social_post ~ GroupAssignment, data = filter(evans_wide, \n    GroupAssignment != \"Direct\"))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.70037 -0.59054  0.05946  0.64963  1.34963 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             4.490541   0.087155   51.52   &lt;2e-16 ***\nGroupAssignmentIndirect 0.009833   0.122931    0.08    0.936    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.845 on 187 degrees of freedom\nMultiple R-squared:  3.422e-05, Adjusted R-squared:  -0.005313 \nF-statistic: 0.006398 on 1 and 187 DF,  p-value: 0.9363\n\n                             2.5 %    97.5 %\n(Intercept)              4.3186067 4.6624745\nGroupAssignmentIndirect -0.2326772 0.2523439\n\n\nIn Evans et al. (2023), the direct vs control comparison is significant, but they approached the analysis differently. They control for pre-test scores by using it as an additional predictor (known as a covariate) in an ANCOVA procedure.\n\n\n\n\n2.5.3 Hypothesis 3\n\nFinally, the third hypothesis focuses on the difference between the two contact groups. They predict that the direct contact group will lead to higher well-being and lower ill-being compared to the indirect contact group. For this question, we focus on loneliness post-test for a measure of ill-being, so we expect lower scores in the direct group.\n\n\n\n\n\n\n\nTry this\n\n\n\nThink about what techniques would let you test the hypothesis that the direct group decreases loneliness at post-test compared to the indirect group.\n\n\n\n\n\n\n\n\nShow me the task list\n\n\n\n\n\nFor this analysis, we focus on the final combination of groups, this time ignoring the control group.\nTo test the difference between two groups on one outcome, we need a model suitable for a between-subjects design. You will have covered linear regression with one categorical predictor for how to test the difference in an outcome between two groups.\n\nFit a linear model on the outcome loneliness post-test with a categorical predictor of group. Filter the data to just focus on the direct and indirect contact groups.\nLook at the intercept, what is the predicted value for the reference group? Look at the slope, what is the estimated change to the target group? For hypothesis 3, is the direct group lower on loneliness compared to indirect? For hypothesis testing, is the p-value lower than alpha?\nWhat is the effect size? How much did the two groups differ on loneliness? What is the confidence interval around the effect size?\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nIn the code below, we first fit a linear model on loneliness post-test and focus on the direct and indirect groups. There are different ways you could ignore the indirect group, but here we filter the data as we pass the data to the lm() function.\nThe direct group decreased loneliness post-test on average 0.14 points compared to the indirect group, but the difference was not statistically significant (\\(b_1\\) = 0.14, 95% CI = [-0.01, 0.29], p = .060).\n\nlm_contact &lt;- lm(lonely_post ~ GroupAssignment, \n                 data = filter(evans_wide,\n                               GroupAssignment != \"Control\"))\n\nsummary(lm_contact)\n\nconfint(lm_contact)\n\n\nCall:\nlm(formula = lonely_post ~ GroupAssignment, data = filter(evans_wide, \n    GroupAssignment != \"Control\"))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96457 -0.42130 -0.06457  0.32645  1.58543 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              1.82355    0.05267  34.625   &lt;2e-16 ***\nGroupAssignmentIndirect  0.14102    0.07448   1.893   0.0598 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5133 on 188 degrees of freedom\nMultiple R-squared:  0.01871,   Adjusted R-squared:  0.01349 \nF-statistic: 3.585 on 1 and 188 DF,  p-value: 0.05983\n\n                               2.5 %    97.5 %\n(Intercept)              1.719655160 1.9274363\nGroupAssignmentIndirect -0.005898489 0.2879484\n\n\nLike hypothesis 2, Evans et al. (2023) report the direct vs indirect comparison as significant, but they approached the analysis differently . They control for pre-test scores by using it as an additional predictor within an ANCOVA type model.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evans et al. (2023): Canine-assisted interventions for wellbeing</span>"
    ]
  },
  {
    "objectID": "Evans-analysis-journey-analysis.html#conclusion",
    "href": "Evans-analysis-journey-analysis.html#conclusion",
    "title": "2  Evans et al. (2023): Canine-assisted interventions for wellbeing",
    "section": "\n2.6 Conclusion",
    "text": "2.6 Conclusion\nWell done! Hopefully you recognised how far your skills have come to be able to do this independently, regardless of how many hints you needed. If you are curious, you can read Evans et al. (2023) to see how they walk through wrangling and analysing the data, and they have some great features like highlighting common student mistakes.\n\n\n\n\nBinfet, J.-T., Green, F. L. L., & Draper, Z. A. (2022). The Importance of Client–Canine Contact in Canine-Assisted Interventions: A Randomized Controlled Trial. Anthrozoös, 35(1), 1–22. https://doi.org/10.1080/08927936.2021.1944558\n\n\nEvans, C., Cipolli, W., Draper, Z. A., & Binfet, J.-T. (2023). Repurposing a Peer-Reviewed Publication to Engage Students in Statistics: An Illustration of Study Design, Data Collection, and Analysis. Journal of Statistics and Data Science Education, 0(0), 1–21. https://doi.org/10.1080/26939169.2023.2238018",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evans et al. (2023): Canine-assisted interventions for wellbeing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bartlett, J. E., Jenks, R., & Wilson, N. (2022). No\nMeaningful Difference in\nAttentional Bias Between\nDaily and Non-Daily\nSmokers. Journal of Trial & Error. https://doi.org/10.36850/e11\n\n\nBinfet, J.-T., Green, F. L. L., & Draper, Z. A. (2022). The\nImportance of Client–Canine\nContact in Canine-Assisted\nInterventions: A Randomized\nControlled Trial. Anthrozoös,\n35(1), 1–22. https://doi.org/10.1080/08927936.2021.1944558\n\n\nEvans, C., Cipolli, W., Draper, Z. A., & Binfet, J.-T. (2023).\nRepurposing a Peer-Reviewed\nPublication to Engage Students in\nStatistics: An Illustration of\nStudy Design, Data\nCollection, and Analysis. Journal of\nStatistics and Data Science Education, 0(0), 1–21. https://doi.org/10.1080/26939169.2023.2238018",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]